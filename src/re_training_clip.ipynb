{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19eca49-0b53-491e-bc55-0a218ef4b4bc",
   "metadata": {},
   "source": [
    "# 実行環境の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f52304a-5b58-4fc7-9e48-3b404ea57102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ce85e-30a4-49a1-8d4b-306b9ccc367b",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdd01f5-4408-4722-ada1-82d3fdef39f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 2)) (0.12.0+cu113)\n",
      "Collecting webdataset>=0.2.5\n",
      "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.7/749.7 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 6)) (4.64.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 7)) (1.3.5)\n",
      "Collecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.9.0->-r ../requirements-training.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (9.1.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from webdataset>=0.2.5->-r ../requirements-training.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->-r ../requirements-training.txt (line 5)) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r ../requirements-training.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r ../requirements-training.txt (line 7)) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->-r ../requirements-training.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (1.26.9)\n",
      "Installing collected packages: braceexpand, webdataset, regex, ftfy\n",
      "Successfully installed braceexpand-0.1.7 ftfy-6.1.1 regex-2022.4.24 webdataset-0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements-training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "469a6cad-f3d2-459d-ac24-2eba2057b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b51bb4-f45f-433f-88c0-ddd1fa30778f",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99153d07-9bc5-42b4-97dd-00f8feb117fe",
   "metadata": {},
   "source": [
    "## データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cb2f643-aea8-4258-89ab-8f58a887320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import (\n",
    "    create_model_and_transforms,\n",
    "    image_transform,\n",
    "    tokenize,\n",
    ")\n",
    "from training import data as data_module\n",
    "from training.data import (\n",
    "    get_data,\n",
    "    get_wds_dataset,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d7589f8-7fe8-4bec-a74e-b38fd78ceb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data_df = pd.read_table(\"./Train_GCC-training.tsv\", header=None)\n",
    "data_df = pd.read_table(\"./Validation_GCC-1.1.0-Validation.tsv\", header=None)\n",
    "data_df = data_df.sample(frac=1, random_state=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f1b1190b-55f4-421a-b049-c23942ab7c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cherry blossoms and a field of roses make for ...</td>\n",
       "      <td>https://static1.squarespace.com/static/504c2ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a sq ft tiny house made from reclaimed barn wo...</td>\n",
       "      <td>https://s-media-cache-ak0.pinimg.com/originals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>person at the fashion show</td>\n",
       "      <td>https://media.gettyimages.com/photos/martha-wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hospitality business also boasts a bird 's eye...</td>\n",
       "      <td>http://i.dailymail.co.uk/i/pix/2016/12/27/21/3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>person in hooded sweater using a laptop on woo...</td>\n",
       "      <td>https://media.gettyimages.com/photos/person-in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  cherry blossoms and a field of roses make for ...   \n",
       "1  a sq ft tiny house made from reclaimed barn wo...   \n",
       "2                         person at the fashion show   \n",
       "3  hospitality business also boasts a bird 's eye...   \n",
       "4  person in hooded sweater using a laptop on woo...   \n",
       "\n",
       "                                                   1  \n",
       "0  https://static1.squarespace.com/static/504c2ec...  \n",
       "1  https://s-media-cache-ak0.pinimg.com/originals...  \n",
       "2  https://media.gettyimages.com/photos/martha-wa...  \n",
       "3  http://i.dailymail.co.uk/i/pix/2016/12/27/21/3...  \n",
       "4  https://media.gettyimages.com/photos/person-in...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b833cbe4-2bfd-47cc-8a3e-5b9a36347ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "画像の読み込み失敗\n",
      "http://www.standard.net/image/2015/02/04/800x_a16-9_b0_q81_p1/winter-fly-fishing.jpg\n",
      "画像の読み込み失敗\n",
      "http://indianapolis-photos.funcityfinder.com/files/2009/12/Clearwater-Crossing-Shopping-Center-sign-Indianapolis-Indiana.jpg\n",
      "画像の読み込み失敗\n",
      "https://www.featurepics.com/StockImage/20090316/carrying-globe-stock-image-1115085.jpg\n",
      "画像の読み込み失敗\n",
      "http://www.waste360.com/sites/waste360.com/files/styles/article_featured_standard/public/Trista%2002%20007_0.jpg?itok=F1eJZsX3\n",
      "画像の読み込み失敗\n",
      "https://media.gettyimages.com/photos/young-woman-seated-on-the-beach-picture-id97545987?s=612x612\n",
      "画像の読み込み失敗\n",
      "http://piquemagazine.uk/wp-content/uploads/2017/10/LPO-24-Feb-Albrecht-Menzel-%C2%AE-Anne-Hornemann-300dpi.jpg\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "model, _, preprocess = create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "\n",
    "batch_num = 10\n",
    "data_cnt = 0\n",
    "images = []\n",
    "texts = []\n",
    "for row_cnt, (text, image_url) in data_df.iterrows():\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(requests.get(image_url).content))\n",
    "    except:\n",
    "        print(\"画像の読み込み失敗\")\n",
    "        print(image_url)\n",
    "        continue\n",
    "\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    images.append(image)\n",
    "    texts.append(text)\n",
    "\n",
    "    if data_cnt == batch_num - 1:\n",
    "        break\n",
    "    else:\n",
    "        data_cnt += 1\n",
    "\n",
    "images = torch.cat(images)\n",
    "texts = tokenize(texts)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "    print(image_features.shape)\n",
    "    print(text_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dc14-fd04-4799-9f6b-67e3a786b56e",
   "metadata": {},
   "source": [
    "# ネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e9cd85e-c429-4b37-a125-d06bcdc45cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f2af1-46ac-4325-ba9e-59c209027936",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b13ef8c-cf4b-420b-b268-efcfdfca7f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3547, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomClipLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, image_features, text_features, logit_scale):\n",
    "        # print(\"logit_scale: \", logit_scale)\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        logits_per_text = logit_scale * text_features @ image_features.T\n",
    "        \n",
    "        # print(logits_per_image.shape)\n",
    "        # print(logits_per_text.shape)\n",
    "\n",
    "        num_logits = logits_per_image.shape[0]\n",
    "        # print(\"num_logits\", num_logits)\n",
    "        labels = torch.arange(num_logits, device=\"cpu\", dtype=torch.long)\n",
    "        # print(\"label: \", labels)\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels) +\n",
    "            F.cross_entropy(logits_per_text, labels)\n",
    "            ) / 2\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "clip_loss = CustomClipLoss()\n",
    "clip_loss(image_features, text_features, logit_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36802f0-7c82-46be-ac62-57e261b51e2a",
   "metadata": {},
   "source": [
    "## Skip Lanyer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fceb218d-db4b-4fd1-b6e6-678f4741a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = torch.add(h, x)\n",
    "        h = self.relu1(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "class SkipNetwork(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, input_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.block1 = Block(input_dim)\n",
    "        self.block2 = Block(input_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(input_dim, output_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3ff00bf3-aceb-4edf-9c9c-28fd65f658e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "データの確認 cherry blossoms and a field of roses make for a classic feminine scene celebrating perfume .\n",
      "train loss:  tensor(3.6071, grad_fn=<DivBackward0>)\n",
      "test loss:  tensor(2.8353)\n",
      "データの確認 cherry blossoms and a field of roses make for a classic feminine scene celebrating perfume .\n",
      "train loss:  tensor(3.4811, grad_fn=<DivBackward0>)\n",
      "test loss:  tensor(2.7836)\n",
      "データの確認 cherry blossoms and a field of roses make for a classic feminine scene celebrating perfume .\n",
      "train loss:  tensor(3.0681, grad_fn=<DivBackward0>)\n",
      "test loss:  tensor(2.7679)\n",
      "データの確認 cherry blossoms and a field of roses make for a classic feminine scene celebrating perfume .\n",
      "train loss:  tensor(2.7798, grad_fn=<DivBackward0>)\n",
      "test loss:  tensor(2.7503)\n",
      "データの確認 cherry blossoms and a field of roses make for a classic feminine scene celebrating perfume .\n",
      "train loss:  tensor(2.2913, grad_fn=<DivBackward0>)\n",
      "test loss:  tensor(2.7236)\n"
     ]
    }
   ],
   "source": [
    "text_network = SkipNetwork(512, 512)\n",
    "image_network = SkipNetwork(512, 512)\n",
    "clip_loss = CustomClipLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(text_network.parameters()) + \n",
    "                             list(image_network.parameters()), lr=0.001)\n",
    "model, _, preprocess = create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "\n",
    "batch_num = 50\n",
    "epoch_num = 5\n",
    "\n",
    "for epoch_cnt in range(epoch_num):\n",
    "    if epoch_cnt == 0:\n",
    "        print(epoch_cnt)\n",
    "\n",
    "    data_cnt = 0\n",
    "    train_images = []\n",
    "    train_texts = []\n",
    "    test_images = []\n",
    "    test_texts = []\n",
    "    for row_cnt, (text, image_url) in data_df.iterrows():\n",
    "        if row_cnt == 0:\n",
    "            print(\"データの確認\", text)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(io.BytesIO(requests.get(image_url, timeout=(3.0, 7.5)).content))\n",
    "        except:\n",
    "            # print(\"画像の読み込み失敗\")\n",
    "            # print(image_url)\n",
    "            continue\n",
    "\n",
    "        image = preprocess(image).unsqueeze(0)\n",
    "\n",
    "        if data_cnt < (batch_num * 2 / 3):\n",
    "            train_images.append(image)\n",
    "            train_texts.append(text)\n",
    "        else:\n",
    "            test_images.append(image)\n",
    "            test_texts.append(text)\n",
    "\n",
    "        if data_cnt == batch_num - 1:\n",
    "            break\n",
    "        else:\n",
    "            data_cnt += 1\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_images = torch.cat(train_images)\n",
    "        train_texts = tokenize(train_texts)\n",
    "        train_image_features, train_text_features, train_logit_scale = model(train_images, train_texts)\n",
    "\n",
    "        test_images = torch.cat(test_images)\n",
    "        test_texts = tokenize(test_texts)\n",
    "        test_image_features, test_text_features, test_logit_scale = model(test_images, test_texts)\n",
    "\n",
    "    train_image_features = image_network(train_image_features)\n",
    "    train_text_features = text_network(train_text_features)\n",
    "    train_loss = clip_loss(train_image_features, train_text_features, train_logit_scale)\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_image_features = image_network(test_image_features)\n",
    "        test_text_features = text_network(test_text_features)\n",
    "        test_loss = clip_loss(test_image_features, test_text_features, test_logit_scale)\n",
    "\n",
    "    print(\"train loss: \", train_loss)\n",
    "    print(\"test loss: \", test_loss)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
