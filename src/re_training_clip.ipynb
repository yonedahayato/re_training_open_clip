{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19eca49-0b53-491e-bc55-0a218ef4b4bc",
   "metadata": {},
   "source": [
    "# 実行環境の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f52304a-5b58-4fc7-9e48-3b404ea57102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ce85e-30a4-49a1-8d4b-306b9ccc367b",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcdd01f5-4408-4722-ada1-82d3fdef39f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 2)) (0.12.0+cu113)\n",
      "Collecting webdataset>=0.2.5\n",
      "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2022.4.24-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.7/749.7 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 6)) (4.64.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r ../requirements-training.txt (line 7)) (1.3.5)\n",
      "Collecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.9.0->-r ../requirements-training.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (9.1.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->-r ../requirements-training.txt (line 2)) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from webdataset>=0.2.5->-r ../requirements-training.txt (line 3)) (6.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.7/site-packages (from ftfy->-r ../requirements-training.txt (line 5)) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r ../requirements-training.txt (line 7)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r ../requirements-training.txt (line 7)) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->-r ../requirements-training.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->-r ../requirements-training.txt (line 2)) (3.3)\n",
      "Installing collected packages: braceexpand, webdataset, regex, ftfy\n",
      "Successfully installed braceexpand-0.1.7 ftfy-6.1.1 regex-2022.4.24 webdataset-0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements-training.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "469a6cad-f3d2-459d-ac24-2eba2057b172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b51bb4-f45f-433f-88c0-ddd1fa30778f",
   "metadata": {},
   "source": [
    "# 検証"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99153d07-9bc5-42b4-97dd-00f8feb117fe",
   "metadata": {},
   "source": [
    "## データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4cb2f643-aea8-4258-89ab-8f58a887320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_clip import (\n",
    "    create_model_and_transforms,\n",
    "    image_transform,\n",
    "    tokenize,\n",
    ")\n",
    "from training import data as data_module\n",
    "from training.data import (\n",
    "    get_data,\n",
    "    get_wds_dataset,\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0d7589f8-7fe8-4bec-a74e-b38fd78ceb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_table(\"./Train_GCC-training.tsv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1190b-55f4-421a-b049-c23942ab7c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a very typical bus station</td>\n",
       "      <td>http://lh6.ggpht.com/-IvRtNLNcG8o/TpFyrudaT6I/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sierra looked stunning in this top and this sk...</td>\n",
       "      <td>http://78.media.tumblr.com/3b133294bdc7c7784b7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>young confused girl standing in front of a war...</td>\n",
       "      <td>https://media.gettyimages.com/photos/young-con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interior design of modern living room with fir...</td>\n",
       "      <td>https://thumb1.shutterstock.com/display_pic_wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cybernetic scene isolated on white background .</td>\n",
       "      <td>https://thumb1.shutterstock.com/display_pic_wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                         a very typical bus station   \n",
       "1  sierra looked stunning in this top and this sk...   \n",
       "2  young confused girl standing in front of a war...   \n",
       "3  interior design of modern living room with fir...   \n",
       "4    cybernetic scene isolated on white background .   \n",
       "\n",
       "                                                   1  \n",
       "0  http://lh6.ggpht.com/-IvRtNLNcG8o/TpFyrudaT6I/...  \n",
       "1  http://78.media.tumblr.com/3b133294bdc7c7784b7...  \n",
       "2  https://media.gettyimages.com/photos/young-con...  \n",
       "3  https://thumb1.shutterstock.com/display_pic_wi...  \n",
       "4  https://thumb1.shutterstock.com/display_pic_wi...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b833cbe4-2bfd-47cc-8a3e-5b9a36347ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([1, 3, 224, 224])\n",
      "画像の読み込み失敗\n",
      "http://www.robinhoodshow.com/clients/17668/8642054_org.jpg\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.Size([10, 512])\n",
      "torch.Size([10, 512])\n"
     ]
    }
   ],
   "source": [
    "model, _, preprocess = create_model_and_transforms('ViT-B-32-quickgelu', pretrained='laion400m_e32')\n",
    "\n",
    "# preprocess = image_transform(128, is_train=False)\n",
    "\n",
    "batch_num = 10\n",
    "data_cnt = 0\n",
    "images = []\n",
    "texts = []\n",
    "for row_cnt, (text, image_url) in data_df.iterrows():\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(requests.get(image_url).content))\n",
    "    except:\n",
    "        print(\"画像の読み込み失敗\")\n",
    "        print(image_url)\n",
    "        continue\n",
    "\n",
    "    image = preprocess(image).unsqueeze(0)\n",
    "    images.append(image)\n",
    "    texts.append(text)\n",
    "\n",
    "    if data_cnt == batch_num - 1:\n",
    "        break\n",
    "    else:\n",
    "        data_cnt += 1\n",
    "\n",
    "images = torch.cat(images)\n",
    "texts = tokenize(texts)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "    print(image_features.shape)\n",
    "    print(text_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3dc14-fd04-4799-9f6b-67e3a786b56e",
   "metadata": {},
   "source": [
    "# ネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e9cd85e-c429-4b37-a125-d06bcdc45cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f2af1-46ac-4325-ba9e-59c209027936",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b13ef8c-cf4b-420b-b268-efcfdfca7f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "class CustomClipLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, image_features, text_features, logit_scale):\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        logits_per_text = logit_scale * text_features @ image_features.T\n",
    "\n",
    "        num_logits = logits_per_image.shape[0]\n",
    "        print(num_logits)\n",
    "        labels = torch.arange(num_logits, device=\"cpu\", dtype=torch.long)\n",
    "        print(labels)\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels) +\n",
    "            F.cross_entropy(logits_per_text, labels)\n",
    "            ) / 2\n",
    "        \n",
    "        print(total_loss)\n",
    "\n",
    "loss = CustomClipLoss()\n",
    "loss(image_features, text_features, logit_scale)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
